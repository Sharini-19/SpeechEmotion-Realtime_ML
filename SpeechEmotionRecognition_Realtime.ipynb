{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sharini-19/SpeechEmotion-Realtime_ML/blob/main/SpeechEmotionRecognition_Realtime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONCzDk-2dNjd",
        "outputId": "11ecde2a-fc61-43b5-e063-cffa12700836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ffmpeg-python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os,time,librosa,warnings,glob\n",
        "import regex as re\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import librosa.display\n",
        "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense,Input,Add,Flatten,Dropout,Activation,AveragePooling1D,Conv1D\n",
        "from keras.models import Model,Sequential,load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler,EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from IPython.display import Audio,HTML\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COLnf0TNdjcH",
        "outputId": "9cb21e0f-d3c4-4d38-cff2-71112f6a887b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_6BMygFnqQR"
      },
      "outputs": [],
      "source": [
        "def calc_time(func):\n",
        "  def inner(*args, **kwargs):\n",
        "    st = time.time()\n",
        "    result = func(*args,**kwargs)\n",
        "    end = time.time()-st\n",
        "    print(\"Total time required: {:.3f} ms\".format(end * 1000))\n",
        "    return result\n",
        "  return inner\n",
        "\n",
        "def ravdess_data():\n",
        "  ravdess = \"/content/drive/MyDrive/Audiofiles/Audiofiles/audio_speech_actors_01-24/\"\n",
        "  emotion_ravdess = {'01':'neutral','02':'calm','03':'happy','04':'sad','05':'angry','06':'fearful','07':'disgust','08':'surprised'}\n",
        "  ravdess_emotion = []\n",
        "  ravdess_path = []\n",
        "  ravdess_folder = os.listdir(ravdess)\n",
        "  for i in ravdess_folder:\n",
        "    inner_files = os.listdir(ravdess+i+'/')\n",
        "    for j in inner_files:\n",
        "      emotion = j.split('-')[2]\n",
        "      ravdess_path.append(ravdess+i+'/'+j)\n",
        "      ravdess_emotion.append(emotion_ravdess[emotion])\n",
        "\n",
        "  df_ravdess = pd.DataFrame([ravdess_path,ravdess_emotion]).T\n",
        "  df_ravdess.columns = [\"AudioPath\",\"Label\"]\n",
        "  print(\"length of ravdess dataset\",len(df_ravdess))\n",
        "\n",
        "  return df_ravdess\n",
        "\n",
        "def crema_data():\n",
        "  #directory of the audio dataset\n",
        "  crema = \"/content/drive/MyDrive/Audiofiles/Audiofiles/AudioWAV/AudioWAV/\"\n",
        "  #label ravdess data\n",
        "  emotion_crema = {'SAD':'sad','ANG':'angry','DIS':'disgust','FEA':'fear','HAP':'happy','NEU':'neutral'}\n",
        "  #list to store crema emotion\n",
        "  crema_emotion = []\n",
        "  #list to store crema audio path\n",
        "  crema_path = []\n",
        "  #get crema files in directory\n",
        "  crema_files = os.listdir(crema)\n",
        "  for i in crema_files:\n",
        "    emotion = i.split('_')[2]\n",
        "    crema_emotion.append(emotion_crema[emotion])\n",
        "    crema_path.append(crema+i)\n",
        "\n",
        "  #convert to dataframe\n",
        "  df_crema = pd.DataFrame([crema_path,crema_emotion]).T\n",
        "  df_crema.columns = [\"AudioPath\",\"Label\"]\n",
        "  print(\"length of crema dataset\",len(df_crema))\n",
        "\n",
        "  return df_crema\n",
        "\n",
        "\n",
        "def saveee_data():\n",
        "  savee = \"/content/drive/MyDrive/Audiofiles/Audiofiles/ALL/ALL/\"\n",
        "  emotion_savee = {'a':'anger','d':'disgust','f':'fear','h':'happiness','n':'neutral','sa':'sadness','su':'surprise'}\n",
        "  savee_emotion = []\n",
        "  savee_path = []\n",
        "  savee_files = os.listdir(savee)\n",
        "  for i in savee_files:\n",
        "    emotion = i.split('_')[1]\n",
        "    emotion = re.match(r\"([a-z]+)([0-9]+)\",emotion)[1]\n",
        "    savee_emotion.append(emotion_savee[emotion])\n",
        "    savee_path.append(savee+i)\n",
        "  #convert to dataframe\n",
        "  df_savee = pd.DataFrame([savee_path,savee_emotion]).T\n",
        "  df_savee.columns = [\"AudioPath\",\"Label\"]\n",
        "  print(\"length of savee dataset\",len(df_savee))\n",
        "\n",
        "  return df_savee\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mExUYipHhOX1"
      },
      "outputs": [],
      "source": [
        "@calc_time\n",
        "def fetch_data():\n",
        "  #get ravdess data\n",
        "  df_ravdess = ravdess_data()\n",
        "  df_crema = crema_data()\n",
        "  #get savee data\n",
        "  df_savee = saveee_data()\n",
        "  #combine two datasets into one single dataset and create a dataframe\n",
        "  frames = [df_ravdess,df_crema,df_savee]\n",
        "  final_combined = pd.concat(frames)\n",
        "  final_combined.reset_index(drop=True,inplace=True)\n",
        "  #save the information of datasets with their path and labels into a csv file\n",
        "  final_combined.to_csv(\"/content/drive/MyDrive/preprocesseddata.csv\",index=False,header=True)\n",
        "  print(\"Total length of the dataset is {}\".format(len(final_combined)))\n",
        "  return final_combined\n",
        "\n",
        "def noise(data):\n",
        "  noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "  data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "  return data\n",
        "\n",
        "#fuction to strech audio\n",
        "def stretch(data, rate=0.8):\n",
        "  return librosa.effects.time_stretch(data, rate)\n",
        "\n",
        "#fucntion to shift audio range\n",
        "def shift(data):\n",
        "  shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "  return np.roll(data, shift_range)\n",
        "\n",
        "#function to change pitch\n",
        "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
        "  return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
        "\n",
        "def extract_features(data,sample_rate):\n",
        "\n",
        "  #zero crossing rate\n",
        "  result = np.array([])\n",
        "  zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
        "  result = np.hstack((result, zcr))\n",
        "  #print('zcr',result.shape)\n",
        "\n",
        "  #chroma shift\n",
        "  stft = np.abs(librosa.stft(data))\n",
        "  chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "  result = np.hstack((result, chroma_stft))\n",
        "  #print('chroma',result.shape)\n",
        "\n",
        "  #mfcc\n",
        "  mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
        "  result = np.hstack((result, mfcc))\n",
        "  #print('mfcc',result.shape)\n",
        "\n",
        "  #rmse\n",
        "  rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
        "  result = np.hstack((result, rms))\n",
        "  #print('rmse',result.shape)\n",
        "\n",
        "  #melspectogram\n",
        "  mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
        "  result = np.hstack((result, mel))\n",
        "  #print('mel',result.shape)\n",
        "\n",
        "  #rollof\n",
        "  rollof = np.mean(librosa.feature.spectral_rolloff(y=data, sr=sample_rate).T, axis=0)\n",
        "  result = np.hstack((result, rollof))\n",
        "\n",
        "  centroid = np.mean(librosa.feature.spectral_centroid(y=data, sr=sample_rate).T, axis=0)\n",
        "  result = np.hstack((result, centroid))\n",
        "  #print('centroids',result.shape)\n",
        "\n",
        "  #contrast\n",
        "  contrast = np.mean(librosa.feature.spectral_contrast(y=data, sr=sample_rate).T, axis=0)\n",
        "  result = np.hstack((result, contrast))\n",
        "  #print('contrast',result.shape)\n",
        "\n",
        "  #bandwidth\n",
        "  bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=data, sr=sample_rate).T, axis=0)\n",
        "  result = np.hstack((result, bandwidth))\n",
        "  #print('bandwidth',result.shape)\n",
        "\n",
        "  #tonnetz\n",
        "  tonnetz = np.mean(librosa.feature.tonnetz(y=data, sr=sample_rate).T, axis=0)\n",
        "  result = np.hstack((result, tonnetz))\n",
        "  #print('tonnetz',result.shape)\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "def get_features(path):\n",
        "  #set the duration and offset\n",
        "  #librosa.load takes audio file converts to array and returns array of audio file with its sampling rate\n",
        "  data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
        "\n",
        "  #get audio features without augmentation\n",
        "  res1 = extract_features(data,sample_rate)\n",
        "  result = np.array(res1)\n",
        "\n",
        "  #get audio features with noise\n",
        "  noise_data = noise(data)\n",
        "  res2 = extract_features(noise_data,sample_rate)\n",
        "  result = np.vstack((result, res2))\n",
        "\n",
        "  #get audio features with stretching and pitching\n",
        "  new_data = stretch(data)\n",
        "  data_stretch_pitch = pitch(new_data, sample_rate)\n",
        "  res3 = extract_features(data_stretch_pitch,sample_rate)\n",
        "  result = np.vstack((result, res3))\n",
        "\n",
        "  return result\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ55qn3LrIrm"
      },
      "outputs": [],
      "source": [
        "@calc_time\n",
        "def Audio_features_extract():\n",
        "  #this function is used to fetch the data from all the four datasets\n",
        "  df = fetch_data()\n",
        "  #count is used to keep a check of number of files processed\n",
        "  count = 0\n",
        "  #list to store audio features and their label information\n",
        "  X_data, Y_label = [], []\n",
        "  #zip audio path and label information and then iterate over them\n",
        "  for path, emotion in zip(df[\"AudioPath\"], df[\"Label\"]):\n",
        "    print(\"Number of files processed \",count)\n",
        "    #get the features\n",
        "    #for one audio file it get three sets of features\n",
        "    #original features, features with noise(agumentation) and feature with change in stretch and pitch\n",
        "    #so one audio file generates three output and the label is same for all the outputs\n",
        "    feature = get_features(path)\n",
        "    for ele in feature:\n",
        "      X_data.append(ele)\n",
        "      Y_label.append(emotion)\n",
        "    count+=1\n",
        "\n",
        "  Features = pd.DataFrame(X_data)\n",
        "  #add label information\n",
        "  Features['Label'] = Y_label\n",
        "  #store the extracted features in a csv file\n",
        "  Features.to_csv('/content/drive/MyDrive/Audiofiles/Audiofiles/Audio_features_All_pr.csv',index=False)\n",
        "  Audio_features_extract()\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKQNKCjT_eV4"
      },
      "outputs": [],
      "source": [
        "def plotgraph(history):\n",
        "  plt.figure(figsize=[8,6])\n",
        "  plt.plot(history.history['loss'],'firebrick',linewidth=3.0)\n",
        "  plt.plot(history.history['accuracy'],'turquoise',linewidth=3.0)\n",
        "  plt.legend(['Training loss','Training Accuracy'],fontsize=18)\n",
        "  plt.xlabel('Epochs ',fontsize=16)\n",
        "  plt.ylabel('Loss and Accuracy',fontsize=16)\n",
        "  plt.title('Loss Curves and Accuracy Curves',fontsize=16)\n",
        "\n",
        "def additional_preprocess(filepath):\n",
        "  #read the csv file of extrated features\n",
        "  df = pd.read_csv(filepath)\n",
        "  print(\"\\nlabels or emotions present in dataset\\n\",df[\"Label\"].unique())\n",
        "  #replace label names with name common for each emotion\n",
        "  #this is done to maintain uniformity of label names\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"sadness\", \"sad\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"happiness\", \"happy\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"Fear\", \"fear\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"Sad\", \"sad\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"Pleasant_surprise\", \"surprise\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"pleasant_surprised\", \"surprise\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"surprised\", \"surprise\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"fearful\", \"fear\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"anger\", \"angry\", case = True)\n",
        "\n",
        "  print(\"\\nUnique count of labels or emotions\\n\",df[\"Label\"].value_counts())\n",
        "  #drop labels or emotions which can lead to misclassifications\n",
        "  df.drop((np.where(df['Label'].isin([\"surprise\",\"calm\"]))[0]), inplace = True)\n",
        "  print(\"\\nUnique count of labels or emotions after dropping selected labels\\n\",df[\"Label\"].value_counts())\n",
        "  print(\"\\nlength of the total data is {}\".format(len(df)))\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhk1xgxJldWv"
      },
      "outputs": [],
      "source": [
        "def additional_preprocess(filepath):\n",
        "  #read the csv file of extrated features\n",
        "  df = pd.read_csv(filepath)\n",
        "  print(\"\\nlabels or emotions present in dataset\\n\",df[\"Label\"].unique())\n",
        "  #replace label names with name common for each emotion\n",
        "  #this is done to maintain uniformity of label names\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"sadness\", \"sad\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"happiness\", \"happy\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"Fear\", \"fear\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"Sad\", \"sad\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"Pleasant_surprise\", \"surprise\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"pleasant_surprised\", \"surprise\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"surprised\", \"surprise\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"fearful\", \"fear\", case = True)\n",
        "  df[\"Label\"] = df[\"Label\"].str.replace(\"anger\", \"angry\", case = True)\n",
        "  #drop labels surprized and clam\n",
        "  #these label dosent contain sufficent amount of data and can lead to missclassification\n",
        "  print(\"\\nUnique count of labels or emotions\\n\",df[\"Label\"].value_counts())\n",
        "  #drop labels or emotions which can lead to misclassifications\n",
        "  df.drop((np.where(df['Label'].isin([\"surprise\",\"calm\"]))[0]), inplace = True)\n",
        "  print(\"\\nUnique count of labels or emotions after dropping selected labels\\n\",df[\"Label\"].value_counts())\n",
        "  print(\"\\nlength of the total data is {}\".format(len(df)))\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldAaV9JRjpwk"
      },
      "outputs": [],
      "source": [
        "\n",
        "@calc_time\n",
        "def audio_features_final():\n",
        "  df = additional_preprocess(\"/content/drive/MyDrive/Audiofiles/Audiofiles/Audio_features_All_pr.csv\")\n",
        "  #get all the audio features as numpy array from the dataframe\n",
        "  #last column is label so last column is not fetched only 0to:-1\n",
        "  data=df[df.columns[0:-1]].values\n",
        "  #perform one hot encoding on labels\n",
        "  encoder = OneHotEncoder()\n",
        "  #fetch the last column of labels and perform one hot encoding on them\n",
        "  label=df[\"Label\"].values\n",
        "  label = encoder.fit_transform(np.array(label).reshape(-1,1)).toarray()\n",
        "  #min max scaler is used to normalize the data\n",
        "  scaler = MinMaxScaler()\n",
        "  data=scaler.fit_transform(data)\n",
        "  #split the dataframe into train and test 80% train, 10% validation and 10% test datasets\n",
        "  x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=42,shuffle=True)\n",
        "  x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.50, random_state=42, shuffle=True)\n",
        "  print(\"\\nlength of train data is {}, test data is {} and validation set is {}\".format(len(x_train),len(x_test),len(x_val)))\n",
        "  print(\"\\n shape of train features and label is {}\".format(x_train.shape, y_train.shape))\n",
        "  print(\"\\n shape of test features and label is {}\".format(x_test.shape, y_test.shape))\n",
        "  print(\"\\n shape of validation features and label is {}\".format(x_val.shape,y_val.shape))\n",
        "  return x_train, x_test, y_train, y_test, x_val, y_val, encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1IjoYi7-5z4"
      },
      "outputs": [],
      "source": [
        "@calc_time\n",
        "def emotion_recognition_model(x_train,y_train,x_val,y_val):\n",
        "  #reduce the learning rate if plateau is encountered\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "  #early stopping method is used to montior the loss if there are no significant reductions in loss then halt the training\n",
        "  es = EarlyStopping(monitor='loss', patience=20)\n",
        "  #checkpoint to save the best model with highest validation accuracy\n",
        "  filepath = \"/content/drive/MyDrive/Audiofiles/emotion-recognition.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "  #create a combined list of reduce learning rate, early stopping and checkpoint\n",
        "  callbacks_list = [reduce_lr,es,checkpoint]\n",
        "\n",
        "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
        "    #fucntion is used to create residual blocks and add residual blocks\n",
        "    s = Conv1D(filters, 1, padding=\"same\")(x)\n",
        "    for i in range(conv_num - 1):\n",
        "      x = Conv1D(filters, 3, padding=\"same\")(x)\n",
        "      x = Activation(activation)(x)\n",
        "      x = Conv1D(filters, 3, padding=\"same\")(x)\n",
        "      x = Add()([x, s])\n",
        "      x = Activation(activation)(x)\n",
        "    return x\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YHpSqZEkAX0"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    inputs =  Input(shape=(x_train.shape[1],1))\n",
        "    x = Dense(256, activation=\"relu\")(inputs)\n",
        "    x = residual_block(x, 16, 2)\n",
        "    x = residual_block(x, 32, 2)\n",
        "    x = residual_block(x, 32, 2)\n",
        "    x = residual_block(x, 64, 3)\n",
        "    x = residual_block(x, 64, 3)\n",
        "    x = residual_block(x, 128, 3)\n",
        "    x = residual_block(x, 128, 3)\n",
        "    #perform the average pooling after last residual block\n",
        "    x = AveragePooling1D(pool_size=3, strides=3)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    outputs = Dense(6, activation=\"softmax\", name=\"output\")(x)\n",
        "    return Model(inputs=inputs, outputs=outputs)\n",
        "    res_model = build_model()\n",
        "  #display the summary of the model\n",
        "    res_model.summary()\n",
        "  #complie the model\n",
        "    res_model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=1e-4, decay=1e-4 / 50) , metrics=['accuracy'])\n",
        "    history = res_model.fit(np.expand_dims(x_train,-1),y_train,\n",
        "                validation_data=(np.expand_dims(x_val, -1), y_val),\n",
        "                epochs=500,\n",
        "                batch_size=32,\n",
        "                shuffle=True,\n",
        "                workers=50,\n",
        "                verbose=1,\n",
        "                use_multiprocessing=True,\n",
        "                callbacks = callbacks_list)\n",
        "\n",
        "  #plot loss and accuracy curves\n",
        "    plotgraph(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OazFbJcDKSK"
      },
      "outputs": [],
      "source": [
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };\n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "#fucntion is used to invoke microphone on laptop and record the voice\n",
        "#it used javascript to work as colab dosent have inbuilt method to record audio\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oez1YxwelHMJ"
      },
      "outputs": [],
      "source": [
        "#function is used to get the audio features recorded from the microphone\n",
        "def get_features_recorded(data,sr):\n",
        "\n",
        "  #get features for recorded audio using mircophone\n",
        "  res1 = extract_features(data,sr)\n",
        "  result = np.array(res1)\n",
        "\n",
        "  #get audio features with noise\n",
        "  noise_data = noise(data)\n",
        "  res2 = extract_features(noise_data,sr)\n",
        "  result = np.vstack((result, res2))\n",
        "\n",
        "  #get audio features with stretching and pitching\n",
        "  new_data = stretch(data)\n",
        "  data_stretch_pitch = pitch(new_data, sr)\n",
        "  res3 = extract_features(data_stretch_pitch,sr)\n",
        "  result = np.vstack((result, res3))\n",
        "\n",
        "  return result\n",
        "\n",
        "#fucntion is used to evaualte performance of model on recorded audio using microphone\n",
        "@calc_time\n",
        "def test_realtime(encoder):\n",
        "\n",
        "  #load the best model\n",
        "  res_model = load_model(\"/content/drive/MyDrive/Audiofiles/emotion-recognition.hdf5\")\n",
        "\n",
        "  #record the aduio\n",
        "  audio, sr = get_audio()\n",
        "\n",
        "  #plot the recorded audio\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.plot(audio)\n",
        "  plt.show()\n",
        "  #save the plot of audio file\n",
        "  #plt.savefig(\"audiorec{}.png\".format(audiofile))\n",
        "  #convert int to float\n",
        "  audio = audio.astype('float')\n",
        "  #get audio features from the recorded voice\n",
        "  feature = get_features_recorded(audio,sr)\n",
        "  #apply min max scaling\n",
        "  scaler = MinMaxScaler()\n",
        "  feature = scaler.fit_transform(feature)\n",
        "  #get the predicted label\n",
        "  label=res_model.predict(feature)\n",
        "  #get the label information by reversing one hot encoded output\n",
        "  label_predicted=encoder.inverse_transform(label)\n",
        "  print(\"\\nThe Emotion Predicted For Recorded Audio Using Microphone is {}\".format(label_predicted[0]))\n",
        "  if(label_predicted [0]=='happy'):\n",
        "    print(\"The recommended learning mode is 1\")\n",
        "  elif(label_predicted[0]=='angry'):\n",
        "    print(\"The recommended learning mode is 2\")\n",
        "  elif(label_predicted[0]=='sad'):\n",
        "    print(\"The recommended learning mode is 3\")\n",
        "  elif(label_predicted[0]=='disgust'):\n",
        "    print(\"The recommended learning mode is 4\")\n",
        "  elif(label_predicted[0]=='neutral'):\n",
        "    print(\"The recommended learning mode is 5\")\n",
        "  elif(label_predicted[0]=='surprised'):\n",
        "    print(\"The recommended learning mode is 6\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkTgDXn8aVoo"
      },
      "outputs": [],
      "source": [
        "#function to evaluate the model performance once the best model is saved\n",
        "#it loads the best model and then evaluates the performance\n",
        "@calc_time\n",
        "def evaluate_model(x_train, x_test, y_train, y_test, x_val, y_val):\n",
        "  #load the best model\n",
        "  model = load_model(\"/content/drive/MyDrive/Audiofiles/emotion-recognition.hdf5\")\n",
        "  #evaluate training accuracy\n",
        "  _,train_acc = model.evaluate(np.expand_dims(x_train,-1),y_train, batch_size=1)\n",
        "  #evaluate testing acuracy\n",
        "  _,test_acc = model.evaluate(np.expand_dims(x_test,-1),y_test, batch_size=1)\n",
        "  #evaluate validation accuracy\n",
        "  _,val_acc = model.evaluate(np.expand_dims(x_val,-1),y_val, batch_size=1)\n",
        "  print(\"\\n**********************************************\")\n",
        "  print(\"\\n Training accuracy of the model is {}\".format(np.round(float(train_acc*100),2)))\n",
        "  print(\"\\n Testing accuracy of the model is {}\".format(np.round(float(test_acc*100),2)))\n",
        "  print(\"\\n Validation accuracy of the model is {}\".format(np.round(float(val_acc*100),2)))\n",
        "  print(\"**********************************************\")\n",
        "  #predict the outcome of the model\n",
        "  y_pred = model.predict(x_test)\n",
        "  y_pred=np.argmax(y_pred, axis=1)\n",
        "  y_test=np.argmax(y_test, axis=1)\n",
        "  #View the classification report for test data and predictions\n",
        "  print(\"\\nClassification report for Emotion Recognition\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  #View confusion matrix for test data and predictions\n",
        "  print(\"\\nConfusion matrix for Emotion Recognition\")\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  print(\"*****************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkCceRw2l_UY"
      },
      "outputs": [],
      "source": [
        "@calc_time\n",
        "def emotion_recognition_model(x_train,y_train,x_val,y_val):\n",
        "  #reduce the laerning rate if plateau is encountered\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "  #early stopping method is used to montior the loss if there are no significant reductions in loss then halt the training\n",
        "  es = EarlyStopping(monitor='loss', patience=20)\n",
        "  #checkpoint to save the best model with highest validation accuracy\n",
        "  filepath = \"/content/drive/MyDrive/Audiofiles/emotion-recognition.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "  #create a combined list of reduce learning rate, early stopping and checkpoint\n",
        "  callbacks_list = [reduce_lr,es,checkpoint]\n",
        "  res_model = build_model()\n",
        "  #display the summary of the model\n",
        "  res_model.summary()\n",
        "  #complie the model\n",
        "  res_model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=1e-4, decay=1e-4 / 50) , metrics=['accuracy'])\n",
        "  history = res_model.fit(np.expand_dims(x_train,-1),y_train,\n",
        "                validation_data=(np.expand_dims(x_val, -1), y_val),\n",
        "                epochs=500,\n",
        "                batch_size=32,\n",
        "                shuffle=True,\n",
        "                workers=50,\n",
        "                verbose=1,\n",
        "                use_multiprocessing=True,\n",
        "                callbacks = callbacks_list)\n",
        "\n",
        "  #plot loss and accuracy curves\n",
        "  plotgraph(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-YNKgrdlseW"
      },
      "outputs": [],
      "source": [
        "@calc_time\n",
        "def evaluate_model(x_train, x_test, y_train, y_test, x_val, y_val):\n",
        "  #load the best model\n",
        "  model = load_model(\"/content/drive/MyDrive/Audiofiles/emotion-recognition.hdf5\")\n",
        "  #evaluate training accuracy\n",
        "  _,train_acc = model.evaluate(np.expand_dims(x_train,-1),y_train, batch_size=1)\n",
        "  #evaluate testing acuracy\n",
        "  _,test_acc = model.evaluate(np.expand_dims(x_test,-1),y_test, batch_size=1)\n",
        "  #evaluate validation accuracy\n",
        "  _,val_acc = model.evaluate(np.expand_dims(x_val,-1),y_val, batch_size=1)\n",
        "  print(\"\\n**********************************************\")\n",
        "  print(\"\\n Training accuracy of the model is {}\".format(np.round(float(train_acc*100),2)))\n",
        "  print(\"\\n Testing accuracy of the model is {}\".format(np.round(float(test_acc*100),2)))\n",
        "  print(\"\\n Validation accuracy of the model is {}\".format(np.round(float(val_acc*100),2)))\n",
        "  print(\"**********************************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSQmqHBotjRD",
        "outputId": "1bd8772a-ac6d-4477-f142-498c8be93f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "labels or emotions present in dataset\n",
            " ['neutral' 'calm' 'happy' 'sad' 'fearful' 'angry' 'surprised' 'disgust'\n",
            " 'fear' 'anger' 'happiness' 'sadness' 'surprise']\n",
            "\n",
            "Unique count of labels or emotions\n",
            " disgust     4587\n",
            "fear        4575\n",
            "sad         4572\n",
            "angry       4572\n",
            "happy       4569\n",
            "neutral     3927\n",
            "surprise     756\n",
            "calm         576\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Unique count of labels or emotions after dropping selected labels\n",
            " disgust    4587\n",
            "fear       4575\n",
            "sad        4572\n",
            "angry      4572\n",
            "happy      4569\n",
            "neutral    3927\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "length of the total data is 26802\n",
            "\n",
            "length of train data is 21441, test data is 2680 and validation set is 2681\n",
            "\n",
            " shape of train features and label is (21441, 178)\n",
            "\n",
            " shape of test features and label is (2680, 178)\n",
            "\n",
            " shape of validation features and label is (2681, 178)\n",
            "Total time required: 3284.480 ms\n",
            "21441/21441 [==============================] - 243s 11ms/step - loss: 0.2273 - accuracy: 0.9202\n",
            "2680/2680 [==============================] - 30s 11ms/step - loss: 1.6383 - accuracy: 0.6216\n",
            "2681/2681 [==============================] - 28s 10ms/step - loss: 1.6516 - accuracy: 0.6330\n",
            "\n",
            "**********************************************\n",
            "\n",
            " Training accuracy of the model is 92.02\n",
            "\n",
            " Testing accuracy of the model is 62.16\n",
            "\n",
            " Validation accuracy of the model is 63.3\n",
            "**********************************************\n",
            "Total time required: 303274.868 ms\n",
            "Total time required: 306560.001 ms\n"
          ]
        }
      ],
      "source": [
        "@calc_time\n",
        "def main():\n",
        "  #get train,test data and labels\n",
        "  x_train, x_test, y_train, y_test, x_val, y_val, encoder = audio_features_final()\n",
        "  #call the emotion recognition model\n",
        "\n",
        "  evaluate_model(x_train,x_test,y_train,y_test,x_val,y_val)\n",
        "\n",
        "\n",
        "if __name__:main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRCC2UgV3Hv1",
        "outputId": "d14df7a4-ae5e-49c0-dad2-2ea6fc9ac741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "labels or emotions present in dataset\n",
            " ['neutral' 'calm' 'happy' 'sad' 'fearful' 'angry' 'surprised' 'disgust'\n",
            " 'fear' 'anger' 'happiness' 'sadness' 'surprise']\n",
            "\n",
            "Unique count of labels or emotions\n",
            " disgust     4587\n",
            "fear        4575\n",
            "sad         4572\n",
            "angry       4572\n",
            "happy       4569\n",
            "neutral     3927\n",
            "surprise     756\n",
            "calm         576\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Unique count of labels or emotions after dropping selected labels\n",
            " disgust    4587\n",
            "fear       4575\n",
            "sad        4572\n",
            "angry      4572\n",
            "happy      4569\n",
            "neutral    3927\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "length of the total data is 26802\n",
            "\n",
            "length of train data is 21441, test data is 2680 and validation set is 2681\n",
            "\n",
            " shape of train features and label is (21441, 178)\n",
            "\n",
            " shape of test features and label is (2680, 178)\n",
            "\n",
            " shape of validation features and label is (2681, 178)\n",
            "Total time required: 1526.877 ms\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test, x_val, y_val, encoder = audio_features_final()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwskPau03UwE",
        "outputId": "2dad6683-7829-4c20-9d34-64796af921cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "one hot encoding array\n",
            " [[0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "One hot encoding mapping to actual label\n",
            " [['sad']\n",
            " ['neutral']\n",
            " ['happy']\n",
            " ['fear']\n",
            " ['disgust']\n",
            " ['angry']]\n"
          ]
        }
      ],
      "source": [
        "#mapping of the one hot encoding with respect to their labels\n",
        "print(\"\\none hot encoding array\\n\",np.unique(y_train,axis=0))\n",
        "print(\"\\nOne hot encoding mapping to actual label\\n\",encoder.inverse_transform(np.unique(y_train,axis=0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eNbwIaSH3_X",
        "outputId": "a8a7064f-d16a-438d-c0b9-1d0e07236b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script>\n",
              "var my_div = document.createElement(\"DIV\");\n",
              "var my_p = document.createElement(\"P\");\n",
              "var my_btn = document.createElement(\"BUTTON\");\n",
              "var t = document.createTextNode(\"Press to start recording\");\n",
              "\n",
              "my_btn.appendChild(t);\n",
              "//my_p.appendChild(my_btn);\n",
              "my_div.appendChild(my_btn);\n",
              "document.body.appendChild(my_div);\n",
              "\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "var recordButton = my_btn;\n",
              "\n",
              "var handleSuccess = function(stream) {\n",
              "  gumStream = stream;\n",
              "  var options = {\n",
              "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
              "    mimeType : 'audio/webm;codecs=opus'\n",
              "    //mimeType : 'audio/webm;codecs=pcm'\n",
              "  };            \n",
              "  //recorder = new MediaRecorder(stream, options);\n",
              "  recorder = new MediaRecorder(stream);\n",
              "  recorder.ondataavailable = function(e) {            \n",
              "    var url = URL.createObjectURL(e.data);\n",
              "    var preview = document.createElement('audio');\n",
              "    preview.controls = true;\n",
              "    preview.src = url;\n",
              "    document.body.appendChild(preview);\n",
              "\n",
              "    reader = new FileReader();\n",
              "    reader.readAsDataURL(e.data); \n",
              "    reader.onloadend = function() {\n",
              "      base64data = reader.result;\n",
              "      //console.log(\"Inside FileReader:\" + base64data);\n",
              "    }\n",
              "  };\n",
              "  recorder.start();\n",
              "  };\n",
              "\n",
              "recordButton.innerText = \"Recording... press to stop\";\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "\n",
              "\n",
              "function toggleRecording() {\n",
              "  if (recorder && recorder.state == \"recording\") {\n",
              "      recorder.stop();\n",
              "      gumStream.getAudioTracks()[0].stop();\n",
              "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
              "  }\n",
              "}\n",
              "\n",
              "// https://stackoverflow.com/a/951057\n",
              "function sleep(ms) {\n",
              "  return new Promise(resolve => setTimeout(resolve, ms));\n",
              "}\n",
              "\n",
              "var data = new Promise(resolve=>{\n",
              "//recordButton.addEventListener(\"click\", toggleRecording);\n",
              "recordButton.onclick = ()=>{\n",
              "toggleRecording()\n",
              "\n",
              "sleep(2000).then(() => {\n",
              "  // wait 2000ms for the data to be available...\n",
              "  // ideally this should use something like await...\n",
              "  //console.log(\"Inside data:\" + base64data)\n",
              "  resolve(base64data.toString())\n",
              "\n",
              "});\n",
              "\n",
              "}\n",
              "});\n",
              "      \n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd470bf1ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 473ms/step\n",
            "\n",
            "The Emotion Predicted For Recorded Audio Using Microphone is ['angry']\n",
            "The recommended learning mode is 2\n",
            "Total time required: 8138.124 ms\n"
          ]
        }
      ],
      "source": [
        "test_realtime(encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FRBvOJ6TE_9"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cgqgFCUljoiJnNmI-pM_ulpshel9BTg6",
      "authorship_tag": "ABX9TyMLNfnM2z6HhRMCcZ3w4fqL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}